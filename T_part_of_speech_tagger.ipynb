{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-04T02:58:36.041783900Z",
     "start_time": "2023-11-04T02:58:34.871573600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "\t\"\"\"Read tagged sentence data\"\"\"\n",
    "\tfrom collections import namedtuple, OrderedDict\n",
    "\tSentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\tcontent = f.read()\n",
    "\t\tsentences = content.split(\"\\n\\n\")\n",
    "\t\tsentences_split_by_words = [sen.split(\"\\n\") for sen in sentences]\n",
    "\n",
    "\tstored_dict = OrderedDict()\n",
    "\tfor sentence in sentences_split_by_words:\n",
    "\t\t# try:\n",
    "\t\t# \tassert sentence[0][0] == 'b'\n",
    "\t\t# except Exception: \n",
    "\t\t# \tcontinue\n",
    "\t\tword_tags = []\n",
    "\t\tfor word in sentence[1:]:\n",
    "\t\t\tword_tag = word.split(\"\\t\")\n",
    "\t\t\t# if word_tag[0] != '.':\n",
    "\t\t\tassert len(word_tag) == 2\n",
    "\t\t\tassert word_tag[1].upper() == word_tag[1]\n",
    "\t\t\tword_tags.append(word_tag)\n",
    "\t\tif len(word_tags) == 0:\n",
    "\t\t\tcontinue\n",
    "\t\ttemp = list(zip(*word_tags))\n",
    "\t\tif len(temp) != 2:\n",
    "\t\t\tpass\n",
    "\t\tassert len(temp) == 2\n",
    "\t\t# print(temp)\n",
    "\t\tl1, l2 = temp\n",
    "\t\ts = Sentence(l1, l2)\n",
    "\t\tstored_dict[sentence[0]] = s\n",
    "\treturn stored_dict\n",
    "\t\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:27:04.308283900Z",
     "start_time": "2023-11-13T07:27:04.295807200Z"
    }
   },
   "id": "b6a55407213d474b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "57340"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_dict = read_data(\"brown-universal.txt\")\n",
    "stored_dict\n",
    "len(stored_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:27:35.113982700Z",
     "start_time": "2023-11-13T07:27:30.530509100Z"
    }
   },
   "id": "8c9c7cc1518064e3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for k in stored_dict:\n",
    "# \tif stored_dict[k].words[0] == '.':\n",
    "# \t\tcount+=1\n",
    "# \t\tprint(stored_dict[k].words)\n",
    "# print(f\"count = {count}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T02:58:39.051597300Z",
     "start_time": "2023-11-04T02:58:39.009374300Z"
    }
   },
   "id": "aee28e6ae8c97916"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_tags(filename):\n",
    "\t\"\"\"Read a list of word tag classes\"\"\"\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\ttags = f.read().split(\"\\n\")\n",
    "\treturn frozenset(tags)\n",
    "\n",
    "tag_set = read_tags('tags-universal.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:28:00.278674600Z",
     "start_time": "2023-11-13T07:28:00.181355300Z"
    }
   },
   "id": "e504f38aaeac89f9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "frozenset({'.',\n           'ADJ',\n           'ADP',\n           'ADV',\n           'CONJ',\n           'DET',\n           'NOUN',\n           'NUM',\n           'PRON',\n           'PRT',\n           'VERB',\n           'X'})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:28:38.417233400Z",
     "start_time": "2023-11-13T07:28:38.274499200Z"
    }
   },
   "id": "81c46fece2a955b6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import namedtuple\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "\tdef __new__(cls, sentences, keys):\n",
    "\t\tword_sequences = tuple([sentences[k].words for k in keys])\n",
    "\t\ttag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "\t\twordset = frozenset(chain(*word_sequences))\n",
    "\t\ttagset = frozenset(chain(*tag_sequences))\n",
    "\t\tN = sum(1 for _ in chain(*word_sequences))\n",
    "\t\tstream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "\t\treturn super().__new__(cls,\n",
    "                               {k: sentences[k] for k in keys},\n",
    "\t\t\t\t\t\t\t   keys,\n",
    "\t\t\t\t\t\t\t   wordset,\n",
    "\t\t\t\t\t\t\t   word_sequences,\n",
    "\t\t\t\t\t\t\t   tagset,\n",
    "\t\t\t\t\t\t\t   tag_sequences,\n",
    "\t\t\t\t\t\t\t   N,\n",
    "\t\t\t\t\t\t\t   stream.__iter__\n",
    "\t\t\t\t\t\t\t   )\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences)\n",
    "\t\n",
    "\tdef __iter__(self):\n",
    "\t\treturn iter(self.sentences.items())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:29:17.783009200Z",
     "start_time": "2023-11-13T07:29:17.682695500Z"
    }
   },
   "id": "a547bdd4002a004f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "\tdef __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "\t\ttagset = read_tags(tagfile)\n",
    "\t\tsentences = read_data(datafile)\n",
    "\t\tkeys = tuple(sentences.keys())\n",
    "\t\twordset = frozenset(chain(*[sentence.words for sentence in sentences.values()]))\n",
    "\t\tword_sequences = tuple([sentences[k].words for k in keys])\n",
    "\t\ttag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "\t\tN = sum(1 for _ in chain(*(sen.words for sen in sentences.values())))\n",
    "\t\n",
    "\t\t# split data into train/test sets\n",
    "\t\t_keys = list(keys)\n",
    "\t\tif seed is not None: \n",
    "\t\t\trandom.seed(seed)\n",
    "\t\trandom.shuffle(_keys)\n",
    "\t\tsplit = int(train_test_split * len(_keys))\n",
    "\t\ttraining_data = Subset(sentences, _keys[:split])\n",
    "\t\ttesting_data = Subset(sentences, _keys[split:])\n",
    "\t\tstream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "\t\treturn super().__new__(cls, dict(sentences), keys,\n",
    "\t\t\t\t\t\t\t   wordset,\n",
    "\t\t\t\t\t\t\t   word_sequences,\n",
    "\t\t\t\t\t\t\t   tagset,\n",
    "\t\t\t\t\t\t\t   tag_sequences,\n",
    "\t\t\t\t\t\t\t   training_data,\n",
    "\t\t\t\t\t\t\t   testing_data,\n",
    "\t\t\t\t\t\t\t   N,\n",
    "\t\t\t\t\t\t\t   stream.__iter__)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences)\n",
    "\t\n",
    "\tdef __iter__(self):\n",
    "\t\treturn iter(self.sentences.items())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:29:23.155515900Z",
     "start_time": "2023-11-13T07:29:22.694874100Z"
    }
   },
   "id": "9bc17bd7e26b725d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"./tags-universal.txt\", \"./brown-universal.txt\", train_test_split=.8)\n",
    "\n",
    "print(f\"There are {len(data)} sentences in the corpus.\")\n",
    "print(f\"There are {len(data.training_set)} sentences in the training set.\")\n",
    "print(f\"There are {len(data.testing_set)} sentences in the testing set.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:29:49.119224100Z",
     "start_time": "2023-11-13T07:29:34.814828400Z"
    }
   },
   "id": "d77b42a430375ea8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "key = 'b100-38532'\n",
    "print(f\"Sentence: {key}\")\n",
    "print(f\"words:\\n\\t{data.sentences[key].words}\")\n",
    "print(f\"tags:\\n\\t{data.sentences[key].tags}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T02:58:45.727187600Z",
     "start_time": "2023-11-04T02:58:45.564758400Z"
    }
   },
   "id": "ec4fd607a8b72619"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are a total of {data.N} samples of {len(data.vocab)} unique words in the corpus.\")\n",
    "print(f\"There are {data.training_set.N} samples of {len(data.training_set.vocab)} unique words in the training set.\")\n",
    "print(f\"There are {data.testing_set.N} samples of {len(data.testing_set.vocab)} unique words in the testing set.\")\n",
    "print(f\"There are {len(data.testing_set.vocab - data.training_set.vocab)} words in the test set that are missing in the training set.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:31:17.067630Z",
     "start_time": "2023-11-13T07:31:16.927392600Z"
    }
   },
   "id": "44e89325674297da"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n"
     ]
    }
   ],
   "source": [
    "# Accessing word and tag sequences\n",
    "for i in range(2):\n",
    "\tprint(f\"Sentence {i+1}: {data.X[i]}\")\n",
    "\tprint(f\"Labels {i+1}: {data.Y[i]}\")\n",
    "\tprint()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:31:33.571431400Z",
     "start_time": "2023-11-13T07:31:33.444666900Z"
    }
   },
   "id": "125bf727e0750490"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n",
      "\t ('and', 'CONJ')\n",
      "\t ('now', 'ADV')\n",
      "\t ('he', 'PRON')\n",
      "\t ('made', 'VERB')\n",
      "\t ('use', 'NOUN')\n",
      "\t ('of', 'ADP')\n",
      "\t ('the', 'DET')\n",
      "\t ('advice', 'NOUN')\n",
      "\t ('.', '.')\n",
      "\t ('But', 'CONJ')\n",
      "\t ('there', 'PRT')\n",
      "\t ('seemed', 'VERB')\n",
      "\t ('to', 'PRT')\n",
      "\t ('be', 'VERB')\n",
      "\t ('some', 'DET')\n",
      "\t ('difference', 'NOUN')\n",
      "\t ('of', 'ADP')\n",
      "\t ('opinion', 'NOUN')\n",
      "\t ('as', 'ADP')\n",
      "\t ('to', 'ADP')\n"
     ]
    }
   ],
   "source": [
    "# Accessing (word, tag) samples\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "\tprint(\"\\t\", pair)\n",
    "\tif i > 25:\n",
    "\t\tbreak"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:31:50.146599800Z",
     "start_time": "2023-11-13T07:31:49.947970Z"
    }
   },
   "id": "2908e50addc0a146"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple tagger\n",
    "### Simply choose the tag most frequently assigned to each word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acc319e53a3a1b39"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def pair_counts(sequences_A, sequences_B):\n",
    "\t\"\"\"\n",
    "\tReturns a dictionary keyed to each unique value in the first sequence list that counts the number of occurrences of the corresponding value from the second sequences list.\n",
    "\tFor example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "\n",
    "\t\"\"\"\n",
    "\tpair_count = {}\n",
    "\tassert len(sequences_A) == len(sequences_B)\n",
    "\tfor w1, w2 in zip(chain(*sequences_A), chain(*sequences_B)):\n",
    "\t\tif w1 in pair_count:\n",
    "\t\t\tif w2 in pair_count[w1]:\n",
    "\t\t\t\tpair_count[w1][w2] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpair_count[w1][w2] = 1\n",
    "\t\telse:\n",
    "\t\t\tpair_count[w1] = dict()\n",
    "\t\t\tpair_count[w1][w2] = 1\n",
    "\treturn pair_count\n",
    "\n",
    "emission_counts = pair_counts(data.training_set.Y, data.training_set.X)\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:32:32.466382100Z",
     "start_time": "2023-11-13T07:32:31.353454900Z"
    }
   },
   "id": "3f33549b06a0f4c0"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['ADV', 'NOUN', '.', 'VERB', 'ADP', 'ADJ', 'CONJ', 'DET', 'PRT', 'NUM', 'PRON', 'X'])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_counts.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:34:03.467112600Z",
     "start_time": "2023-11-13T07:34:03.321882500Z"
    }
   },
   "id": "adc9acc31afee5fb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# most-freqent-class tagger\n",
    "# find the most frequent class label for each word in the training data\n",
    "# MFCTagger class is meant to be similar to the interface of the Pomegranate HMM models so that they can be used interchangeably\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "\t\n",
    "\tmissing = FakeState(name=\"<MISSING>\")\n",
    "\t\n",
    "\tdef __init__(self, table):\n",
    "\t\tself.table = defaultdict(lambda: MFCTagger.missing)\n",
    "\t\tself.table.update({\n",
    "\t\t\tword: FakeState(name=tag) for word, tag in table.items()\n",
    "\t\t})\n",
    "\t\n",
    "\tdef viterbi(self, seq):\n",
    "\t\t\"\"\"\n",
    "\t\tThis method simplifies predictions by matching the Pomegranate viterbi() interface\n",
    "\t\t\"\"\"\n",
    "\t\treturn 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\t\n",
    "word_counts = pair_counts(data.training_set.Y, data.training_set.X)\n",
    "mfc_table = {}\n",
    "for word in data.training_set.vocab:\n",
    "\tmost_frequent_count = -1\n",
    "\tmost_frequent_tag = None\n",
    "\tfor tag in data.training_set.tagset:\n",
    "\t\ttry:\n",
    "\t\t\tcount = word_counts[tag][word]\n",
    "\t\t\tif count > most_frequent_count:\n",
    "\t\t\t\tmost_frequent_count = count\n",
    "\t\t\t\tmost_frequent_tag = tag\n",
    "\t\t\t\tmfc_table[word] = most_frequent_tag \n",
    "\t\texcept KeyError:\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:35:57.551907100Z",
     "start_time": "2023-11-13T07:35:52.115517100Z"
    }
   },
   "id": "d02760ff5e6b76c0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poverty-stricken: ADJ\n",
      "styled: VERB\n",
      "payroll: NOUN\n",
      "spinach: NOUN\n",
      "they: PRON\n",
      "rushes: VERB\n",
      "Future: ADJ\n",
      "Hammer: NOUN\n",
      "$14,000: NOUN\n",
      "aborigine: NOUN\n",
      "Sheriff: NOUN\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word, tag in mfc_table.items():\n",
    "\tprint(f\"{word}: {tag}\")\n",
    "\ti += 1\n",
    "\tif i > 10: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:38:00.398898300Z",
     "start_time": "2023-11-13T07:38:00.183299700Z"
    }
   },
   "id": "9b747d16075fe940"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "\t\"\"\"Return a copy of the input sequence where each unknown word is replaced by the literal string value 'nan'. Pomegranate will ignore these vvalues during computation.\"\"\"\n",
    "\treturn [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "\t\"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "\t_, state_path = model.viterbi(replace_unknown(X))\n",
    "\treturn [state[1].name for state in state_path[1:-1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:38:40.723390900Z",
     "start_time": "2023-11-13T07:38:40.564687500Z"
    }
   },
   "id": "da17b1dffbe10d94"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "Sentence: ('and', 'August', '15', ',', 'November', '15', ',', 'February', '17', ',', 'and', 'May', '15', ',', '(', 'Cranston', ')', '.')\n",
      "Predicted labels: \n",
      "-------------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "Sentence: ('She', 'had', 'the', 'opportunity', 'that', 'few', 'clever', 'women', 'can', 'resist', ',', 'of', 'showing', 'her', 'superiority', 'in', 'argument', 'over', 'a', 'man', '.')\n",
      "Predicted labels: \n",
      "-------------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "Sentence: ('The', 'same', 'conclusions', 'can', 'be', 'drawn', 'from', 'the', 'other', 'physical', 'evidence', 'of', 'the', 'Dark', 'ages', ',', 'from', 'linguistic', 'distribution', ',', 'and', 'from', 'the', 'survivals', 'of', 'early', 'social', ',', 'political', ',', 'and', 'religious', 'patterns', 'into', 'later', 'ages', '.')\n",
      "Predicted labels: \n",
      "-------------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n"
     ]
    }
   ],
   "source": [
    "# Example decoding sequences with MFC Tagger\n",
    "mfc_model = MFCTagger(mfc_table)\n",
    "for key in data.testing_set.keys[:3]:\n",
    "\tprint(f\"Sentence Key: {key}\")\n",
    "\tprint(f\"Sentence: {data.sentences[key].words}\")\n",
    "\tprint(f\"Predicted labels: \\n-------------------\")\n",
    "\tprint(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "\tprint()\n",
    "\tprint(\"Actual labels:\\n--------------------\")\n",
    "\tprint(data.sentences[key].tags)\n",
    "\tprint()\n",
    "\t\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:38:48.553826500Z",
     "start_time": "2023-11-13T07:38:48.341721700Z"
    }
   },
   "id": "a732b7992576c058"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Evaluating model accuracy\n",
    "def accuracy(X, Y, model):\n",
    "\t\"\"\"\n",
    "\tCalculate the prediction accuracy by using the model to decode each sequence in the input X and comparing the prediction with the true labels in Y.\n",
    "\t\n",
    "\tThe X should be an array whose first dimensions is the number of sentences to test, and each element of the array should be an iterable of the words in the sequence. The arrays X and Y should have the exact same shape.\n",
    "\t\"\"\"\n",
    "\tcorrect = total_predictions = 0\n",
    "\tfor observations, actual_tags in zip(X, Y):\n",
    "\t\t# if there's any exception, count the full sentence as an error (which makes this a conservative estimate)\n",
    "\t\ttry:\n",
    "\t\t\tmost_likely_tags = simplify_decoding(observations, model)\n",
    "\t\t\tcorrect += sum(p==t for p, t in zip(most_likely_tags, actual_tags))\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\ttotal_predictions += len(observations)\n",
    "\treturn correct / total_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:39:46.508526400Z",
     "start_time": "2023-11-13T07:39:46.376771Z"
    }
   },
   "id": "779b9730314ca6"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.72%\n",
      "testing accuracy mfc_model: 93.02%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div class=\"alert alert-block alert-success\">MFC tagger accuracy looks correct!</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the accuracy of the MFC tagger\n",
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(f\"training accuracy mfc_model: {100 * mfc_training_acc:.2f}%\")\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(f\"testing accuracy mfc_model: {100 * mfc_testing_acc:.2f}%\")\n",
    "\n",
    "assert mfc_training_acc >= 0.955\n",
    "assert mfc_testing_acc >= 0.925\n",
    "from IPython.core.display import HTML\n",
    "HTML('<div class=\"alert alert-block alert-success\">MFC tagger accuracy looks correct!</div>')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:40:04.089324300Z",
     "start_time": "2023-11-13T07:40:00.843861800Z"
    }
   },
   "id": "84288edb5e95acad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hidden Markov Model tagger\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "816b5dd244b0721a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ADV': 44877,\n 'NOUN': 220632,\n '.': 117757,\n 'VERB': 146161,\n 'ADP': 115808,\n 'ADJ': 66754,\n 'CONJ': 30537,\n 'DET': 109671,\n 'PRT': 23906,\n 'NUM': 11878,\n 'PRON': 39383,\n 'X': 1094}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigram_counts(sequences):\n",
    "\t\"\"\"Return a dictionary keyed to each unique value in the input sequence list that counts the number of occurrences of the value in the sequences list.  The sequences collection should be a 2-dimensional array.\n",
    "\t\n",
    "\tFor example, if the tag NOUN appears 275558 times over all the input sequences, then should return a dictionary such that dict[NOUN] == 275558\"\"\"\n",
    "\td = {}\n",
    "\tfor seq in sequences:\n",
    "\t\tfor t in seq:\n",
    "\t\t\tif t in d:\n",
    "\t\t\t\td[t] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\td[t] = 1\n",
    "\treturn d\n",
    "\n",
    "tag_unigrams = unigram_counts(data.training_set.Y)\n",
    "tag_unigrams\n",
    "# len(data.training_set.Y)\n",
    "# sum(tag_unigrams.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:40:29.007973500Z",
     "start_time": "2023-11-13T07:40:26.819544300Z"
    }
   },
   "id": "1701a73c6edafdf"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{('ADV', 'NOUN'): 1478,\n ('NOUN', '.'): 62639,\n ('.', 'ADV'): 5124,\n ('ADV', '.'): 7577,\n ('.', 'VERB'): 9041,\n ('VERB', 'ADP'): 24927,\n ('ADP', 'ADJ'): 9533,\n ('ADJ', 'NOUN'): 43664,\n ('NOUN', 'CONJ'): 13185,\n ('CONJ', 'VERB'): 6012,\n ('VERB', 'ADJ'): 8423,\n ('.', 'DET'): 8008,\n ('DET', 'VERB'): 7062,\n ('ADJ', 'PRT'): 1301,\n ('PRT', 'ADP'): 2189,\n ('ADP', 'NUM'): 3467,\n ('NUM', 'NOUN'): 4524,\n ('.', 'PRON'): 5448,\n ('PRON', 'VERB'): 27860,\n ('VERB', 'PRT'): 9556,\n ('PRT', 'VERB'): 14886,\n ('VERB', 'NOUN'): 14230,\n ('NOUN', 'NUM'): 1783,\n ('NUM', '.'): 3210,\n ('.', 'NUM'): 1412,\n ('.', '.'): 12588,\n ('ADP', 'ADV'): 1805,\n ('ADV', 'NUM'): 597,\n ('DET', 'NOUN'): 68785,\n ('CONJ', 'DET'): 4636,\n ('NOUN', 'VERB'): 34972,\n ('ADP', 'NOUN'): 29965,\n ('ADP', 'DET'): 52841,\n ('NOUN', 'ADP'): 53884,\n ('CONJ', 'NOUN'): 7502,\n ('.', 'NOUN'): 9782,\n ('VERB', '.'): 11699,\n ('VERB', 'VERB'): 26957,\n ('.', 'ADP'): 7595,\n ('ADV', 'DET'): 3309,\n ('DET', 'ADJ'): 26236,\n ('NOUN', 'DET'): 3425,\n ('ADJ', '.'): 6666,\n ('VERB', 'DET'): 23764,\n ('ADJ', 'VERB'): 1167,\n ('NOUN', 'NOUN'): 32990,\n ('PRT', 'DET'): 2021,\n ('VERB', 'ADV'): 15076,\n ('ADV', 'CONJ'): 789,\n ('NOUN', 'ADJ'): 2839,\n ('DET', '.'): 1385,\n ('ADV', 'ADV'): 4336,\n ('ADV', 'ADJ'): 6143,\n ('ADJ', 'ADP'): 5895,\n ('ADP', 'PRON'): 8109,\n ('ADP', 'ADP'): 2347,\n ('NOUN', 'PRON'): 4369,\n ('ADV', 'PRT'): 1305,\n ('ADJ', 'ADJ'): 3758,\n ('.', 'ADJ'): 3334,\n ('ADJ', 'ADV'): 645,\n ('VERB', 'PRON'): 8001,\n ('PRON', '.'): 4078,\n ('.', 'CONJ'): 8174,\n ('CONJ', 'PRON'): 2058,\n ('DET', 'ADV'): 1937,\n ('ADV', 'VERB'): 10835,\n ('ADV', 'ADP'): 6352,\n ('CONJ', 'ADV'): 2759,\n ('VERB', 'CONJ'): 2105,\n ('ADP', 'VERB'): 4690,\n ('NOUN', 'ADV'): 5804,\n ('ADP', '.'): 1099,\n ('CONJ', 'ADJ'): 3372,\n ('VERB', 'NUM'): 1320,\n ('ADP', 'PRT'): 1675,\n ('PRON', 'DET'): 695,\n ('PRT', 'NUM'): 117,\n ('NUM', 'NUM'): 267,\n ('PRON', 'PRON'): 318,\n ('PRON', 'ADV'): 2099,\n ('DET', 'NUM'): 1073,\n ('PRON', 'ADP'): 2216,\n ('DET', 'PRON'): 1093,\n ('NUM', 'VERB'): 537,\n ('NUM', 'ADP'): 1559,\n ('.', 'PRT'): 2168,\n ('NOUN', 'PRT'): 3946,\n ('DET', 'ADP'): 978,\n ('ADV', 'PRON'): 2136,\n ('CONJ', 'ADP'): 2222,\n ('PRT', '.'): 1794,\n ('PRON', 'NOUN'): 340,\n ('ADJ', 'CONJ'): 2500,\n ('PRT', 'NOUN'): 845,\n ('ADP', 'CONJ'): 217,\n ('DET', 'DET'): 662,\n ('NUM', 'ADJ'): 705,\n ('DET', 'PRT'): 220,\n ('PRT', 'PRT'): 261,\n ('PRON', 'ADJ'): 359,\n ('PRON', 'PRT'): 919,\n ('PRON', 'CONJ'): 455,\n ('ADJ', 'PRON'): 249,\n ('PRT', 'ADV'): 866,\n ('PRT', 'PRON'): 166,\n ('PRT', 'CONJ'): 285,\n ('CONJ', 'PRT'): 760,\n ('CONJ', 'NUM'): 575,\n ('NUM', 'CONJ'): 442,\n ('PRT', 'ADJ'): 467,\n ('X', 'NOUN'): 58,\n ('NUM', 'ADV'): 234,\n ('NOUN', 'X'): 74,\n ('X', 'PRON'): 9,\n ('NUM', 'PRON'): 109,\n ('ADJ', 'NUM'): 467,\n ('NUM', 'DET'): 156,\n ('CONJ', '.'): 612,\n ('X', 'X'): 552,\n ('X', 'VERB'): 62,\n ('DET', 'X'): 156,\n ('PRON', 'NUM'): 39,\n ('DET', 'CONJ'): 70,\n ('X', 'CONJ'): 24,\n ('X', 'ADP'): 61,\n ('X', 'DET'): 5,\n ('X', '.'): 303,\n ('ADJ', 'DET'): 386,\n ('.', 'X'): 147,\n ('ADP', 'X'): 53,\n ('CONJ', 'X'): 18,\n ('ADJ', 'X'): 31,\n ('NUM', 'PRT'): 69,\n ('VERB', 'X'): 28,\n ('X', 'ADV'): 7,\n ('ADV', 'X'): 4,\n ('X', 'PRT'): 8,\n ('NUM', 'X'): 3,\n ('PRT', 'X'): 2,\n ('CONJ', 'CONJ'): 9,\n ('X', 'ADJ'): 3,\n ('X', 'NUM'): 1,\n ('PRON', 'X'): 1}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the transition counts from a certain tag to another type of tag\n",
    "def bigram_counts(sequences):\n",
    "\t\"\"\"Return a dictionary keyed to each unique pair of values in the input sequences list that counts the number of occurrences of pair in the sequences list. The input should be a 2-dimensional array.\"\"\"\n",
    "\td = {}\n",
    "\tfor seq in sequences:\n",
    "\t\tfor idx, t in enumerate(seq[:-1]):\n",
    "\t\t\tpair = (seq[idx], seq[idx+1])\n",
    "\t\t\tif pair in d:\n",
    "\t\t\t\td[pair] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\td[pair] = 1\n",
    "\treturn d\n",
    "\n",
    "tag_bigrams = bigram_counts(data.training_set.Y)\n",
    "tag_bigrams\n",
    "# sum(tag_bigrams.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:40:45.190372700Z",
     "start_time": "2023-11-13T07:40:43.148170700Z"
    }
   },
   "id": "87341dcad50fd49d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ADV': 4185,\n 'ADP': 5583,\n 'ADJ': 1582,\n 'PRT': 1718,\n 'DET': 9763,\n 'PRON': 7318,\n 'NOUN': 6469,\n 'CONJ': 2282,\n '.': 4107,\n 'NUM': 760,\n 'VERB': 2080,\n 'X': 25}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# must also taking into account these \"<start> -> word type\" transitions\n",
    "def starting_counts(sequences):\n",
    "\t\"\"\"Return a dictionary keyed to each value in the input sequences list that counts the number of occurrences where that value is at the beginning of a sequence\"\"\"\n",
    "\td = {}\n",
    "\tfor seq in sequences:\n",
    "\t\tif len(seq) == 0: continue\n",
    "\t\tbeginning_word = seq[0]\n",
    "\t\tif beginning_word in d:\n",
    "\t\t\td[beginning_word] += 1\n",
    "\t\telse:\n",
    "\t\t\td[beginning_word] = 1\n",
    "\treturn d\n",
    "\n",
    "tag_starts = starting_counts(data.training_set.Y)\n",
    "tag_starts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:40:58.235606300Z",
     "start_time": "2023-11-13T07:40:58.149268900Z"
    }
   },
   "id": "3e132a67051f767f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'.': 44936,\n 'NOUN': 722,\n 'NUM': 63,\n 'VERB': 75,\n 'ADJ': 25,\n 'ADV': 16,\n 'ADP': 7,\n 'DET': 14,\n 'CONJ': 2,\n 'PRON': 4,\n 'PRT': 7,\n 'X': 1}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the instances where a sentence with a given tag\n",
    "def ending_counts(sequences):\n",
    "\t\"\"\"Return a dictionary keyed to each unique value in the input sequences list that counts the number of occurrences where that value is at the end of a sequence.\"\"\"\n",
    "\td = {}\n",
    "\tfor seq in sequences:\n",
    "\t\tif len(seq) == 0: continue\n",
    "\t\tending_word = seq[-1]\n",
    "\t\tif ending_word in d:\n",
    "\t\t\td[ending_word] += 1\n",
    "\t\telse:\n",
    "\t\t\td[ending_word] = 1\n",
    "\treturn d\n",
    "\n",
    "tag_ends = ending_counts(data.training_set.Y)\n",
    "tag_ends"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:41:11.617216800Z",
     "start_time": "2023-11-13T07:41:11.534869700Z"
    }
   },
   "id": "e20dd8eb20443391"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# building HMM tagger\n",
    "# the DenseHMM class in pomegranate no longer seems to support viterbi algorithm\n",
    "# so wrap it around MyHMM which supports viterbi\n",
    "from pomegranate.hmm import DenseHMM\n",
    "import numpy as np\n",
    "class MyHMM(DenseHMM):\n",
    "\t\n",
    "\tdef viterbi(self, X_input):\n",
    "\t\timport numpy as np\n",
    "\t\tX_shape = np.array(X_input).shape\n",
    "\t\tif len(X_shape) != 3:\n",
    "\t\t\tX = np.array([[[X_input[i]] for i in range(len(X_input))]])\n",
    "\t\tassert len(X.shape) == 3\n",
    "\t\tassert X.shape[0] == 1\n",
    "\t\tassert X.shape[2] == 1\n",
    "\t\tsequence_length = len(X[0])\n",
    "\t\tassert sequence_length > 0\n",
    "\t\t# l_message = self.forward(X)\n",
    "\t\tnum_states = len(self.distributions)\n",
    "\t\tprevious_max_l = np.array([\n",
    "\t\t\t[None for i in range(sequence_length)]\n",
    "\t\t\tfor j in range(num_states)\n",
    "\t\t])\n",
    "\t\tmax_l_log = np.array([\n",
    "\t\t\t[float(\"-inf\") for i in range(sequence_length)]\n",
    "\t\t\tfor j in range(num_states)\n",
    "\t\t])\n",
    "\t\t# initialize the first \"column\" in viterbi algorithm - looping through each row\n",
    "\t\tfor r in range(num_states):\n",
    "\t\t\tmax_l_log[r][0] = self.starts[r].item() + np.log(list(self.distributions[r].parameters())[1][0][X[0, 0, 0]])\n",
    "\t\t# go through each \"column\"\n",
    "\t\tfor c in range(1, sequence_length):\n",
    "\t\t\t# go through each state in that column, use the values from previous column to update this state in this column\n",
    "\t\t\tfor r in range(num_states):\n",
    "\t\t\t\t# compute the log-probabilities leading to this state in this column\n",
    "\t\t\t\t# temp_arr = []\n",
    "\t\t\t\t# for prev_r in range(num_states):\n",
    "\t\t\t\t# \tprev_l_log = max_l_log[prev_r][c-1]\n",
    "\t\t\t\t# \ttransition_prob_log = self.edges[prev_r][r]\n",
    "\t\t\t\t# \ttemp_arr.append(prev_l_log + transition_prob_log)\n",
    "\t\t\t\t# temp_arr = np.array(temp_arr)\n",
    "\t\t\t\ttemp_arr = np.array([max_l_log[prev_r][c - 1] + self.edges[prev_r][r] for prev_r in range(num_states)])\t\t\t\t\n",
    "\t\t\t\t# determine the max\n",
    "\t\t\t\tprev_r_max = np.argmax(temp_arr)\n",
    "\t\t\t\t# update the values and previous_max_l table\n",
    "\t\t\t\tprob_of_this_observation = list(self.distributions[r].parameters())[1][0][X[0, c, 0]].item()\n",
    "\t\t\t\t\n",
    "\t\t\t\tmax_l_log[r][c] = temp_arr[prev_r_max] + np.log(prob_of_this_observation)\n",
    "\t\t\t\tprevious_max_l[r][c] = prev_r_max\n",
    "\t\t# max_l_log = np.array(max_l_log)\n",
    "\t\tlast_col = max_l_log[:, -1]\n",
    "\t\tmax_row = np.argmax(last_col)\n",
    "\t\t\n",
    "\t\tmost_likely_path = []\n",
    "\t\tmost_likely_path.append(max_row)\n",
    "\t\tcurrent = max_row\n",
    "\t\tfor c in range(sequence_length-1, 0, -1):\n",
    "\t\t\tprev_r = previous_max_l[current][c]\n",
    "\t\t\tassert prev_r is not None\n",
    "\t\t\tmost_likely_path.append(prev_r)\n",
    "\t\t\tcurrent = prev_r\n",
    "\t\t\n",
    "\t\treturn np.exp(last_col[max_row]), most_likely_path[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:06:31.393274500Z",
     "start_time": "2023-11-13T08:06:30.851779800Z"
    }
   },
   "id": "eb3bb908b023f900"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.tagset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:42:01.278841600Z",
     "start_time": "2023-11-13T07:42:01.246402200Z"
    }
   },
   "id": "d7eecc12b06ca08b"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# implementation of a basic HMM tagger\n",
    "\n",
    "from pomegranate.distributions import Categorical\n",
    "# must assign an index/ID to each tag and each word first, because pomegranate needs those indices for distributions or the states\n",
    "dict_id_to_tag = {}\n",
    "dict_tag_to_id = {}\n",
    "for id, tag in enumerate(data.tagset):\n",
    "\tdict_id_to_tag[id] = tag\n",
    "\tdict_tag_to_id[tag] = id\n",
    "dict_id_to_word = {}\n",
    "dict_word_to_id = {}\n",
    "for id, word in enumerate(data.vocab):\n",
    "\tdict_id_to_word[id] = word\n",
    "\tdict_word_to_id[word] = id\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T07:42:22.727444500Z",
     "start_time": "2023-11-13T07:42:22.542289600Z"
    }
   },
   "id": "1171715648ae1811"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thuan Nguyen\\.conda\\envs\\UdacityAI\\Lib\\site-packages\\pomegranate\\_utils.py:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "# emission probability distributions\n",
    "emission_distributions = {}\n",
    "for tag in data.training_set.tagset:\n",
    "\tprobs_for_this_tag = []\n",
    "\tfor word_id in range(len(data.vocab)):\n",
    "\t\tword = dict_id_to_word[word_id]\n",
    "\t\tif word in emission_counts[tag]:\n",
    "\t\t\tprob = emission_counts[tag][word] / tag_unigrams[tag]\n",
    "\t\telse:\n",
    "\t\t\tprob = 10**(-10)\n",
    "\t\tprobs_for_this_tag.append(prob)\n",
    "\tassert len(probs_for_this_tag) == len(data.vocab)\n",
    "\tprobs_for_this_tag = np.array(probs_for_this_tag)\n",
    "\tprobs_for_this_tag = probs_for_this_tag / np.sum(probs_for_this_tag)\n",
    "\tassert abs(sum(probs_for_this_tag) - 1) < 0.00000001 \n",
    "\temission_distributions[tag] = Categorical([probs_for_this_tag])\n",
    "\n",
    "# adding these distributions to hmm model\n",
    "hmm_model = MyHMM()\n",
    "hmm_model.add_distributions([emission_distributions[dict_id_to_tag[i]] for i in range(len(data.training_set.tagset))])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:41:57.732639700Z",
     "start_time": "2023-11-13T08:41:51.157869100Z"
    }
   },
   "id": "8534d05541d51c54"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "num_of_tags = len(data.training_set.tagset)\n",
    "# get the count of total number of \"transitions\" - this is equals to the total number of words - the number of \"ending words,\" essentially: num of all words - num of sentences. This will be the denominator in the transition probabilities\n",
    "count_all_transitions = data.training_set.N - len(data.training_set.X)\n",
    "\n",
    "for tag1_id in range(num_of_tags):\n",
    "\tfor tag2_id in range(num_of_tags):\n",
    "\t\ttag1 = dict_id_to_tag[tag1_id]\n",
    "\t\ttag2 = dict_id_to_tag[tag2_id]\n",
    "\t\t# retrieve the counts of transition from tag1 to tag2\n",
    "\t\ttry:\n",
    "\t\t\tcount_transitions = tag_bigrams[(tag1, tag2)]\n",
    "\t\texcept KeyError:\n",
    "\t\t\tcount_transitions = 1\n",
    "\t\t\tcount_all_transitions += 1\n",
    "\t\t# compute the probability\n",
    "\t\tprob = count_transitions / count_all_transitions\n",
    "\t\t# add the transition probability to the HMM\n",
    "\t\tif prob > 0:\n",
    "\t\t\tdist_tag1 = emission_distributions[tag1]\n",
    "\t\t\tdist_tag2 = emission_distributions[tag2]\n",
    "\t\t\t# print(f\"adding edge between {dist_tag1} and {dist_tag2}: prob = {prob}\")\n",
    "\t\t\thmm_model.add_edge(dist_tag1, dist_tag2, prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:41:57.789533700Z",
     "start_time": "2023-11-13T08:41:57.677742200Z"
    }
   },
   "id": "27790e8c39bdf5f0"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# add edges from starting state to each tag\n",
    "count_sentences = len(data.training_set.X)\n",
    "num_of_tags = len(data.training_set.tagset)\n",
    "start_probs = []\n",
    "for tag_id in range(num_of_tags):\n",
    "\ttag = dict_id_to_tag[tag_id]\n",
    "\tprob = tag_starts[tag] / count_sentences\n",
    "\tstart_probs.append(prob)\n",
    "\tdist_tag = emission_distributions[tag]\n",
    "\thmm_model.add_edge(hmm_model.start, dist_tag, prob)\n",
    "assert abs(sum(start_probs) - 1) < 0.00000001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:42:16.523186600Z",
     "start_time": "2023-11-13T08:42:16.497235800Z"
    }
   },
   "id": "a005799ff8a53688"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# add edges to model.end\n",
    "count_sentences = len(data.training_set.X)\n",
    "num_of_tags = len(data.training_set.tagset)\n",
    "end_probs = []\n",
    "for tag_id in range(num_of_tags):\n",
    "\ttag = dict_id_to_tag[tag_id]\n",
    "\tprob = tag_ends[tag] / count_sentences\n",
    "\tend_probs.append(prob)\n",
    "\tdist_tag = emission_distributions[tag]\n",
    "\thmm_model.add_edge(dist_tag, hmm_model.end, prob)\n",
    "assert abs(sum(end_probs) - 1) < 0.00000001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:42:16.592058900Z",
     "start_time": "2023-11-13T08:42:16.526180900Z"
    }
   },
   "id": "40593b4f97d636d2"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Whenever', 'artists', ',', 'indeed', ',', 'turned', 'to', 'actual', 'representations', 'or', 'molded', 'three-dimensional', 'figures', ',', 'which', 'were', 'rare', 'down', 'to', '800', 'B.C.', ',', 'they', 'tended', 'to', 'reflect', 'reality', '(', 'see', 'Plate', '6a', ',', '9b', ')', ';', ';')\n",
      "['ADV', 'NOUN', '.', 'ADV', '.', 'VERB', 'ADP', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'ADJ', 'NOUN', '.', 'DET', 'VERB', 'ADJ', 'PRT', 'ADP', 'NUM', 'NOUN', '.', 'PRON', 'VERB', 'PRT', 'VERB', 'NOUN', '.', 'VERB', 'NOUN', 'NUM', '.', 'NUM', '.', '.', '.']\n",
      "\n",
      "('For', 'almost', 'two', 'months', ',', 'the', 'defendant', 'and', 'the', 'world', 'heard', 'from', 'individuals', 'escaped', 'from', 'the', 'grave', 'about', 'fathers', 'and', 'mothers', ',', 'graybeards', ',', 'adolescents', ',', 'babies', ',', 'starved', ',', 'beaten', 'to', 'death', ',', 'strangled', ',', 'machine-gunned', ',', 'gassed', ',', 'burned', '.')\n",
      "['ADP', 'ADV', 'NUM', 'NOUN', '.', 'DET', 'NOUN', 'CONJ', 'DET', 'NOUN', 'VERB', 'ADP', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CONJ', 'NOUN', '.', 'NOUN', '.', 'NOUN', '.', 'NOUN', '.', 'VERB', '.', 'VERB', 'ADP', 'NOUN', '.', 'VERB', '.', 'VERB', '.', 'VERB', '.', 'VERB', '.']\n",
      "\n",
      "\n",
      "('Clearer', 'meaning')\n",
      "['ADJ', 'NOUN']\n",
      "\n",
      "\n",
      "('Yes', ',', 'gentlemen', ',', 'I', 'am', 'getting', 'to', 'the', 'point', ',', 'to', 'my', 'point', '.')\n",
      "['ADV', '.', 'NOUN', '.', 'PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "('About', 'the', 'same', 'time', 'the', 'Alleghenies', 'and', 'Poconos', 'in', 'Pennsylvania', 'are', 'magnificent', '--', 'Renovo', 'holds', 'its', 'annual', 'Flaming', 'Foliage', 'Festival', 'on', 'Oct.', '14', ',', '15', '.')\n",
      "['ADV', 'DET', 'ADJ', 'NOUN', 'DET', 'NOUN', 'CONJ', 'NOUN', 'ADP', 'NOUN', 'VERB', 'ADJ', '.', 'NOUN', 'VERB', 'DET', 'ADJ', 'VERB', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NUM', '.', 'NUM', '.']\n"
     ]
    }
   ],
   "source": [
    "# now testing the HMM model\n",
    "\n",
    "xx = data.training_set.X[0]\n",
    "def convert_words_to_ids(xx):\n",
    "\tsentence_converted_to_word_id = [dict_word_to_id[word] for word in xx]\n",
    "\treturn sentence_converted_to_word_id\n",
    "\n",
    "def convert_ids_to_tags(tag_id_list):\n",
    "\tconverted_to_tags = [dict_id_to_tag[tag_id] for tag_id in tag_id_list]\n",
    "\treturn converted_to_tags\n",
    "\n",
    "for i in range(5):\n",
    "\txx = data.training_set.X[i]\n",
    "\t_, tag_id_list = hmm_model.viterbi(convert_words_to_ids(xx))\n",
    "\tprint(xx)\n",
    "\t# print(tag_id_list)\n",
    "\tprint(convert_ids_to_tags(tag_id_list))\n",
    "\tprint()\n",
    "\tprint()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T08:42:18.198071500Z",
     "start_time": "2023-11-13T08:42:16.561615100Z"
    }
   },
   "id": "babbe6e54d1d404b"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# Evaluating model accuracy\n",
    "def hmm_accuracy(X, Y, model):\n",
    "\t\"\"\"\n",
    "\tCalculate the prediction accuracy by using the model to decode each sequence in the input X and comparing the prediction with the true labels in Y.\n",
    "\t\n",
    "\tThe X should be an array whose first dimensions is the number of sentences to test, and each element of the array should be an iterable of the words in the sequence. The arrays X and Y should have the exact same shape.\n",
    "\t\"\"\"\n",
    "\tcorrect = total_predictions = 0\n",
    "\tnum_sentences = len(X)\n",
    "\tfor i, (observations, actual_tags) in enumerate(zip(X, Y)):\n",
    "\t\t# print(f\"Parsing sentence {i} out of {num_sentences}\")\n",
    "\t\t# if there's any exception, count the full sentence as an error (which makes this a conservative estimate)\n",
    "\t\ttry:\n",
    "\t\t\t# most_likely_tags = simplify_decoding(observations, model)\n",
    "\t\t\ttem = convert_words_to_ids(observations)\n",
    "\t\t\t_, tag_id_list = hmm_model.viterbi(tem)\n",
    "\t\t\tmost_likely_tags = convert_ids_to_tags(tag_id_list)\n",
    "\t\t\tcount_correct_this_sentence = sum(p==t for p, t in zip(most_likely_tags, actual_tags))\n",
    "\t\t\t# print(f\"Parsing sentence {i} out of {num_sentences} - correct {count_correct_this_sentence}/{len(actual_tags)} tags\")\n",
    "\t\t\tcorrect += count_correct_this_sentence\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\ttotal_predictions += len(observations)\n",
    "\treturn correct / total_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:02:03.920762700Z",
     "start_time": "2023-11-13T09:02:03.818454400Z"
    }
   },
   "id": "45926ebe4e36b405"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy - basic HMM: 96.66%\n",
      "testing accuracy - basic HMM: 95.46%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = hmm_accuracy(data.training_set.X[:1000], data.training_set.Y[:1000], hmm_model)\n",
    "print(f\"training accuracy - basic HMM: {100 * hmm_training_acc:.2f}%\")\n",
    "\n",
    "hmm_testing_acc = hmm_accuracy(data.testing_set.X[:1000], data.testing_set.Y[:1000], hmm_model)\n",
    "print(f\"testing accuracy - basic HMM: {100 * hmm_testing_acc:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:11:27.527904200Z",
     "start_time": "2023-11-13T09:02:16.625131600Z"
    }
   },
   "id": "167692baaa1576ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
